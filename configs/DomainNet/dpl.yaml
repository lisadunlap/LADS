EXP:
  ADVICE_METHOD: "DPL"
  WANDB_SILENT: False
  PROJ: "DomainNetMini_LADS_Replication"
  SEED: 0
  TEXT_PROMPTS: ['a painting of a {} bird.']
  NEUTRAL_TEXT_PROMPTS: ['a photo of a {} bird.']
  EPOCHS: 20
  CHECKPOINT_VAL: True

DATA:
  DATASET: "DomainNetMini"
  BATCH_SIZE: 256

METHOD:
  MODEL:
    NUM_LAYERS: 1
    LR: 0.001
    WEIGHT_DECAY: 0.05
    CHECKPOINT_NAME: 'dpl'
  BATCH_AVERAGING: True
  TEST_BATCH_AVG: False

OPTIM:
  NAME: "sgd"
  LR: 0.002
  MAX_EPOCH: 10
  LR_SCHEDULER: "cosine"
  WARMUP_EPOCH: 1
  WEIGHT_DECAY: 5e-4
  MOMENTUM: 0.9
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  RMSPROP_ALPHA: 0.99
  # The following also apply to other
  # adaptive optimizers like adamw
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  # STAGED_LR allows different layers to have
  # different lr, e.g. pre-trained base layers
  # can be assigned a smaller lr than the new
  # classification layer
  STAGED_LR: False
  NEW_LAYERS: ()
  BASE_LR_MULT: 0.1
  # -1 or 0 means the stepsize is equal to max_epoch
  STEPSIZE: (-1, )
  GAMMA: 0.1
  # Either linear or constant
  WARMUP_TYPE: "linear"
  # Constant learning rate when type=constant
  WARMUP_CONS_LR: 1e-5
  # Minimum learning rate when type=linear
  WARMUP_MIN_LR: 1e-5
  # Recount epoch for the next scheduler (last_epoch=-1)
  # Otherwise last_epoch=warmup_epoch
  WARMUP_RECOUNT: True

TRAINER:
  COCOOP:
    N_CTX: 4
    AVG: False
    LOAD_CTX: False
    CTX_CHECKPOINT: ''
    NUM_DOM_TOKEN: 4
    CTX_INIT: "a photo of a"
    PREC: "fp16"

INPUT:
  SIZE:
  - 224
  - 224
  INTERPOLATION: "bicubic"
  PIXEL_MEAN: 
  - 0.48145466
  - 0.4578275
  - 0.40821073
  PIXEL_STD: 
  - 0.26862954
  - 0.26130258
  - 0.27577711
  TRANSFORMS: 
  - "random_resized_crop"
  - "random_flip"
  - "normalize"